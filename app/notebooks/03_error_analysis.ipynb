{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 \u2013 Error Analysis\n",
        "\n",
        "Compare predictions vs. gold labels, compute a confusion matrix, and surface top errors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "gold = [\"POSITIVE\",\"NEGATIVE\",\"NEUTRAL\",\"POSITIVE\",\"NEGATIVE\",\"NEGATIVE\"]\n",
        "pred = [\"POSITIVE\",\"NEGATIVE\",\"NEGATIVE\",\"NEUTRAL\",\"NEGATIVE\",\"POSITIVE\"]\n",
        "texts = [\n",
        "    \"Loved it\", \"Hated it\", \"Meh\", \"Pretty good overall\", \"This was awful\", \"Not great\"\n",
        "]\n",
        "\n",
        "labels = sorted(set(gold) | set(pred))\n",
        "cm = confusion_matrix(gold, pred, labels=labels)\n",
        "pd.DataFrame(cm, index=[f\"gold_{l}\" for l in labels], columns=[f\"pred_{l}\" for l in labels])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(classification_report(gold, pred, digits=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Show the mistakes\n",
        "rows = []\n",
        "for g, p, t in zip(gold, pred, texts):\n",
        "    if g != p:\n",
        "        rows.append({\"gold\": g, \"pred\": p, \"text\": t})\n",
        "pd.DataFrame(rows)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}