# ğŸ§  Legal Clause Assistant (Local RAG with Meta-Llama-Guard-2-8B)

This project builds a legal clause retrieval-augmented generation (RAG) pipeline to answer contract-related questions using the [CUAD dataset](https://huggingface.co/datasets/TheAtticusProject/cuad) and a local **Meta-Llama-Guard-2-8B** model.

---

## ğŸ“ Project Structure

```
legal_clause_assistant_llama3/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ ingest.py                  # Ingests and embeds documents into Chroma DB
â”‚   â”œâ”€â”€ qa_chain_llama.py         # LLaMA-based RAG chain logic
â”‚   â”œâ”€â”€ test_qa_chain_llama.py    # Script to test the RAG pipeline
â”‚   â””â”€â”€ test_llama_guard.py       # Standalone model test
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ CUAD_v1/              # CUAD dataset (CSV + TXT + metadata)
â”œâ”€â”€ vector_store/                 # Persisted Chroma DB
â”œâ”€â”€ llamaenv/                     # Python virtual environment
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup

### 1. Clone and create virtualenv

```bash
git clone <repo> legal_clause_assistant_llama3
cd legal_clause_assistant_llama3
python3 -m venv llamaenv
source llamaenv/bin/activate
```

### 2. Install dependencies

```bash
pip install torch transformers accelerate bitsandbytes
pip install sentence-transformers chromadb
pip install langchain langchain-community
```

---

## ğŸ§© CUAD Dataset

- Download CUAD from [Zenodo](https://zenodo.org/record/4595762)
- Unzip into: `data/raw/CUAD_v1/`

We verified documentâ€“metadata alignment using:

```bash
python data/raw/CUAD_v1/test_cuad_v1_data.py
```

---

## ğŸ“¥ Ingest the Corpus

```bash
python app/ingest.py
```

- 510 documents loaded
- ~73,000 chunks generated and embedded
- Stored into `vector_store/chroma.db`

---

## ğŸ§ª Test LLaMA Model

Ensure Hugging Face login:
```bash
huggingface-cli login
```

Then test the model:
```bash
python app/test_llama_guard.py
```

---

## ğŸ” Run RAG Pipeline Locally

```bash
python app/test_qa_chain_llama.py
```

- Retrieves top-5 chunks
- Feeds to Meta-Llama-Guard-2-8B
- Returns answer + metadata

---

## ğŸ” Notes

- We used `transformers` + `bitsandbytes` for 8-bit quantized model loading.
- Metadata mismatch during ingestion was handled via filtering.
- Hugging Face gated repo access was configured via CLI login.

---

## ğŸ”® Next Steps

- Build Gradio UI (`gradio_ui_llama.py`)
- Add FastAPI endpoint (`main_llama.py`)
- Add eval pipeline (`eval_llama.py`)
