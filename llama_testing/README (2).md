# ğŸ§  Legal Clause Assistant (RAG with CUAD + Mistral-7B)

This project builds a local **RAG-based legal question-answering assistant** using the [CUAD dataset](https://github.com/TheAtticusProject/cuad) and the `Mistral-7B-Instruct-v0.1` model (quantized). It allows answering questions about real-world contract clauses using embedded knowledge.

---

## ğŸ”§ Features

- âœ… **CUAD document chunking & vector indexing**
- ğŸ” **Chroma** vector store with `bge-base-en-v1.5` embeddings
- ğŸ¤– **LLM-powered QA pipeline**
  - `OpenAI GPT-4` (optional)
  - âœ… `Mistral-7B-Instruct-v0.1` (local with 4-bit quantization)
- ğŸ§© Prompt templates for customizable instruction formatting
- ğŸ–¼ï¸ Modular code structure for API/UI/eval additions

---

## ğŸ—‚ï¸ Project Structure

```
legal_clause_assistant_llama3/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ ingest.py                   # Chunk + embed CUAD into Chroma
â”‚   â”œâ”€â”€ retriever.py                # Load vector store & retrieve
â”‚   â”œâ”€â”€ qa_chain.py                 # QA pipeline using OpenAI
â”‚   â”œâ”€â”€ qa_chain_mistral.py         # âœ… QA pipeline using Mistral-7B
â”‚   â”œâ”€â”€ prompts.py                  # Prompt templates
â”‚   â”œâ”€â”€ test_qa_chain_openai.py     # Test with OpenAI chain
â”‚   â”œâ”€â”€ test_qa_chain_mistral.py    # âœ… Test with Mistral chain
â”‚   â”œâ”€â”€ test_mistralai.py           # Standalone generation test
â”‚   â”œâ”€â”€ eval.py                     # Evaluation placeholder
â”‚   â””â”€â”€ utils.py                    # Helpers for chunking, cleaning
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # CUAD files (.csv, .txt)
â”‚   â””â”€â”€ processed/                  # Cleaned or normalized content
â”‚
â”œâ”€â”€ vectorstore/                   # Chroma DB directory
â”œâ”€â”€ llmragenv/                     # Python venv (exclude in Git)
â”œâ”€â”€ README.md
â”œâ”€â”€ progress.md                    # ğŸ“ˆ Task and milestone log
â””â”€â”€ requirements.txt
```

---

## ğŸš€ Quickstart

### 1. ğŸ“¦ Install

```bash
python3 -m venv llmragenv
source llmragenv/bin/activate

pip install -r requirements.txt
```

> Ensure you have CUDA + bitsandbytes dependencies installed for GPU support.

---

### 2. ğŸ“„ Prepare Data

- Download CUAD v1 from [Zenodo](https://zenodo.org/record/4592955) or GitHub.
- Place `master_clauses.csv` and `.txt` files into `data/raw/CUAD_v1/`.

---

### 3. ğŸ”Œ Ingest Documents

```bash
python app/ingest.py
```

- This will:
  - Load and chunk contracts
  - Embed them
  - Store in `Chroma` vector DB

---

### 4. ğŸ§ª Test the Model

#### âœ… Test local Mistral model:

```bash
python app/test_mistralai.py
```

#### âœ… Test full QA chain (retriever + Mistral):

```bash
python app/test_qa_chain_mistral.py
```

---

## ğŸ§  Progress Summary

See [`progress.md`](./progress.md) for a full timeline.

Highlights:
- CUAD dataset diagnosed and cleaned
- 73,706 contract chunks embedded with BAAI/bge-base-en-v1.5
- GPT-4 QA chain validated
- ğŸŒ€ Switched to local Mistral for full offline capability
- Vector store + Retriever + Prompt templating integrated

---

## ğŸ› ï¸ Next Steps

- [ ] FastAPI or Gradio UI
- [ ] Evaluation harness (human or heuristic)
- [ ] Prompt tuning and clause-type classification
- [ ] Deployment + Dockerization

---

## ğŸ“š References

- CUAD Dataset: https://github.com/TheAtticusProject/cuad
- Mistral: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1
- LangChain: https://python.langchain.com
